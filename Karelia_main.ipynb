{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNeokTLpDyUuEoGrKG451/P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamzager/Karelia_churn/blob/cleaning/Karelia_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TOKEN = ''\n",
        "URL = f'https://iamzager:{TOKEN}@github.com/iamzager/Karelia_churn.git'"
      ],
      "metadata": {
        "id": "3xy7h88Eqiu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBqjFwms5xv4",
        "outputId": "c6f85ecc-9408-43b7-effa-3f5c09d85044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized empty Git repository in /content/.git/\n",
            "remote: Enumerating objects: 268, done.\u001b[K\n",
            "remote: Counting objects: 100% (268/268), done.\u001b[K\n",
            "remote: Compressing objects: 100% (202/202), done.\u001b[K\n",
            "remote: Total 268 (delta 95), reused 200 (delta 64), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (268/268), 113.14 MiB | 20.74 MiB/s, done.\n",
            "Resolving deltas: 100% (95/95), done.\n",
            "From https://github.com/iamzager/Karelia_churn\n",
            " * branch            cleaning   -> FETCH_HEAD\n"
          ]
        }
      ],
      "source": [
        "!git init\n",
        "!git pull $URL\n",
        "# !git config --global user.email ''\n",
        "!git config --global user.name 'iamzager'\n",
        "!git remote add origin $URL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import cross_val_score, cross_validate, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "import gc\n",
        "import json"
      ],
      "metadata": {
        "id": "PeGBktzneGz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('train.csv').set_index('contract_id')\n",
        "log = pd.read_csv('support_log.csv', parse_dates=['event_date'])\n",
        "type_contract = pd.read_csv('type_contract.csv').drop_duplicates('contract_id')\n",
        "sample = pd.read_csv('sample_solution.csv')\n",
        "sample['blocked'] = np.nan\n",
        "competitors = pd.read_csv('dns_log.csv', parse_dates=['date'], index_col=0).reset_index(drop=True)\n",
        "competitors['date'] = competitors['date'].dt.date\n",
        "events = pd.concat(\n",
        "    [\n",
        "        competitors[['date', 'url', 'contract_id']],\n",
        "        log[['event_date', 'event_type', 'contract_id']].rename({'event_date':'date'},axis=1)\n",
        "    ], axis=0\n",
        "    )\n",
        "events['event'] =  events['event_type'].fillna(events['url'])\n",
        "events = events.drop(['url', 'event_type'], axis=1).sort_values(by=['contract_id', 'date'])"
      ],
      "metadata": {
        "id": "JNSz6E8seG2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 17\n",
        "PREDS_FILE_NAME = 'final_predictions.csv'\n",
        "BEST_PARAMS_FILE_NAME = 'best_params.json'\n",
        "META_BEST_PARAMS_FILE_NAME = 'meta_best_params.json'"
      ],
      "metadata": {
        "id": "7p998DFVDBZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Гиперпараметры подбираются в файле Karelia_tuning"
      ],
      "metadata": {
        "id": "gR3zIh43-OMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !git pull origin\n",
        "with open(BEST_PARAMS_FILE_NAME) as f:\n",
        "    best_params = json.load(f)\n",
        "with open(META_BEST_PARAMS_FILE_NAME) as f:\n",
        "    meta_best_params = json.load(f)"
      ],
      "metadata": {
        "id": "LCjSPJ-6dcqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_BEST_PARAMS = best_params['lgb_params']\n",
        "LOF_BEST_PARAMS = best_params['lof_params']\n",
        "\n",
        "META_VEC_PARAMS = meta_best_params['vec_params']\n",
        "META_ESTIMATOR_PARAMS = meta_best_params['sgd_params']\n",
        "\n",
        "PCA_VEC_PARAMS = {\n",
        "    'ngram_range' : (1,2),\n",
        "    'tokenizer' : lambda s: s.split('__'),\n",
        "    'max_features' : None\n",
        "}        \n",
        "PCA_PARAMS = {\n",
        "    'svd_solver' : 'randomized',\n",
        "    'random_state' : RANDOM_STATE\n",
        "}"
      ],
      "metadata": {
        "id": "FF5vC4LhqCnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Помимо пизнаков, выделенных при разведочном анализе, действия пользователей (events) кодируются с помощью Tf-idf\n",
        "- Данные отсортированы так, чтобы n-граммы представляли собой n действий, выполненных **подряд** в определенном порядке. Таким образом, изучаются характерные цепочки действий, паттерны в поведении пользователей\n",
        "- Векторизованные действия представлены в итоговых признаках двумя способами:\n",
        "    - Сжимаются PCA. Первые 12 компонент используются как признаки pca_i\n",
        "    - Передаются логисической регрессии (SGDClassifier(loss='log')). <br> \n",
        "        В качестве признака **meta** используется прогноз вероятности целевого класса. <br>\n",
        "        Для избежания переобучения реализован классический стекинг с делением на 30 фолдов\n",
        "- Выбросы убираются с помощью LocalOutlierFactor"
      ],
      "metadata": {
        "id": "TQnjUGH8-WDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(X, estimator):    \n",
        "    mask = (estimator.fit_predict(X) == 1)\n",
        "    print(f'{round(1 - (mask.sum() / mask.shape[0]), 2)}%, {mask.shape[0] - mask.sum()} штук выбросов')\n",
        "    return mask\n",
        "\n",
        "def add_payment_features(X):\n",
        "    X = pd.merge(X, type_contract, on='contract_id', how='left')\\\n",
        "        .set_index('contract_id')\n",
        "    X['day_or_month_contract'] = X['day_or_month_contract'].fillna(0).astype('category')\n",
        "    return X\n",
        "\n",
        "def add_url_features(X, competitors):\n",
        "    # Число днс запросов\n",
        "    id_to_url = competitors.groupby('contract_id')[['rt', 'sampo']].sum()\n",
        "    id_to_url['n_urls_log'] = id_to_url.eval('rt + sampo')\n",
        "    X = pd.merge(X, np.log1p(id_to_url), on='contract_id', how='left')\\\n",
        "        .fillna(0)\\\n",
        "        .rename({'rt':'rt_log', 'sampo':'sampo_log'}, axis=1)\n",
        "\n",
        "    # Число запросов за прошлые периоды\n",
        "    train_contracts = X.index.unique()    \n",
        "    comp_max_date = competitors.query('contract_id in @train_contracts')['date'].max()\n",
        "    for week_offset in [1]:\n",
        "        date_limit = comp_max_date - pd.Timedelta(days=week_offset*5)\n",
        "        counts = competitors.query('date >= @date_limit')\\\n",
        "            .groupby('contract_id')['date'].count()\\\n",
        "            .rename(f'n_urls_{week_offset}_log')\n",
        "        X = pd.merge(X, counts, on='contract_id', how='left')\n",
        "        X[f'n_urls_{week_offset}_log'] = np.log1p(X[f'n_urls_{week_offset}_log'].fillna(0))\n",
        "\n",
        "    # Наличие днс запросов\n",
        "    X['has_urls'] = ((X['n_urls_log'] > 0) * 1).astype('category')\n",
        "\n",
        "    # Много запросов\n",
        "    X['has_many_urls'] = ((np.expm1(X['n_urls_log']) > 300) * 1).astype('category')\n",
        "\n",
        "    # Наличие запросов к обоим конкурентам\n",
        "    X['has_both_competitors'] = (X.eval('(rt_log > 0) & (sampo_log > 0)') * 1).astype('category')\n",
        "    X = X.drop(['sampo_log'], axis=1)\n",
        "\n",
        "    # Наличие запросов за прошлые периоды\n",
        "    for week_offset in [1]: \n",
        "        X[f'has_urls_{week_offset}'] = ((X[f'n_urls_{week_offset}_log'] > 0) * 1).astype('category')\n",
        "\n",
        "    # Число дней с прошлого запроса\n",
        "    days_from_last_url = np.log1p(\n",
        "        (\n",
        "            comp_max_date - competitors.groupby('contract_id')['date'].max()\n",
        "        ).dt.days.rename('days_from_last_url_log')\n",
        "    )\n",
        "    X = pd.merge(X, days_from_last_url, on='contract_id', how='left')\n",
        "    X['days_from_last_url_log'] = X['days_from_last_url_log']\\\n",
        "        .fillna(X['days_from_last_url_log'].max())\n",
        "\n",
        "    # Число дней с первого запроса\n",
        "    days_from_first_url = (\n",
        "        comp_max_date - competitors.groupby('contract_id')['date'].min()\n",
        "    ).dt.days.rename('days_from_first_url')\n",
        "    X = pd.merge(X, days_from_first_url, on='contract_id', how='left')\n",
        "    X['days_from_first_url'] = X['days_from_first_url'].fillna(0)\n",
        "\n",
        "    # Наличие запросов к самым популярным и наименее популярным адресам\n",
        "    url_counts = competitors.groupby('url')['url']\\\n",
        "        .count()\\\n",
        "        .sort_values(ascending=False)\n",
        "    top_5_urls = set(url_counts.head(5).index.values)\n",
        "    bottom_urls = set(url_counts[url_counts <= 3].index.values)\n",
        "    X['has_top_urls'] = competitors.groupby('contract_id')['url']\\\n",
        "        .apply(lambda x : (set(x) & top_5_urls) != set()) * 1\n",
        "    X['has_bottom_urls'] = competitors.groupby('contract_id')['url']\\\n",
        "        .apply(lambda x : (set(x) & bottom_urls) != set()) * 1\n",
        "    X[['has_top_urls', 'has_bottom_urls']] = X[['has_top_urls', 'has_bottom_urls']].fillna(0).astype('category')\n",
        "\n",
        "    del id_to_url, counts, days_from_last_url, days_from_first_url,\\\n",
        "        top_5_urls, bottom_urls\n",
        "    gc.collect()\n",
        "    return X\n",
        "\n",
        "def add_support_features(X, support_log):\n",
        "    # Число обращений\n",
        "    X = pd.merge(\n",
        "        X,\n",
        "        support_log.groupby('contract_id')['event_date'].count(), on='contract_id', how='left'\n",
        "        ).rename({'event_date':'n_requests_log'}, axis=1)\n",
        "    X['n_requests_log'] = np.log1p(X['n_requests_log'].fillna(0))\n",
        "\n",
        "    # Число обращений за прошлые периоды\n",
        "    train_contracts = X.index.unique()    \n",
        "    log_max_date = support_log.query('contract_id in @train_contracts')['event_date'].max()\n",
        "    for week_offset in [1]:\n",
        "        date_limit = log_max_date - pd.Timedelta(days=week_offset*5)\n",
        "        counts = support_log.query('event_date >= @date_limit')\\\n",
        "            .groupby('contract_id')['event_date'].count()\n",
        "        X = pd.merge(X, counts, on='contract_id', how='left')\\\n",
        "            .rename({'event_date':f'n_requests_{week_offset}_log'}, axis=1)\n",
        "        X[f'n_requests_{week_offset}_log'] = np.log1p(X[f'n_requests_{week_offset}_log'].fillna(0))\n",
        "\n",
        "    # Наличие обращений\n",
        "    X['has_requests'] = ((X['n_requests_log'] > 0) * 1).astype('category')\n",
        "    for week_offset in [1]: \n",
        "        X[f'has_requests_{week_offset}'] = ((X[f'n_requests_{week_offset}_log'] > 0) * 1).astype('category')\n",
        "\n",
        "    # Много обращений\n",
        "    X['has_many_requests'] = ((np.expm1(X['n_requests_log']) > 5) * 1).astype('category')\n",
        "\n",
        "    # Дней с прошлого обращения\n",
        "    days_from_last_request = (\n",
        "        log_max_date - support_log.groupby('contract_id')['event_date'].max()\n",
        "        ).dt.days.rename('days_from_last_request')\n",
        "    days_from_last_request.name = 'days_from_last_request'\n",
        "    X = pd.merge(X, days_from_last_request, on='contract_id', how='left')\n",
        "    X['days_from_last_request'] = X['days_from_last_request']\\\n",
        "        .fillna(X['days_from_last_request'].max())\\\n",
        "        .astype('int')\n",
        "\n",
        "    # Дней с первого обращения\n",
        "    days_from_first_request = (\n",
        "        log_max_date - support_log.groupby('contract_id')['event_date'].min()\n",
        "    ).dt.days.rename('days_from_first_request')\n",
        "    X = pd.merge(X, days_from_first_request, on='contract_id', how='left')\n",
        "    X['days_from_first_request'] = X['days_from_first_request'].fillna(0)\n",
        "\n",
        "    # Число типов обращений\n",
        "    n_types = support_log.groupby('contract_id')['event_type'].nunique().rename('n_types')\n",
        "    X = pd.merge(X, n_types, on='contract_id', how='left')\n",
        "    X['n_types'] = X['n_types'].fillna(0).astype('int')\n",
        "    \n",
        "    # Повторяющиеся типы\n",
        "    date_splits = [\n",
        "        support_log['event_date'].min(),\n",
        "        log_max_date - pd.Timedelta(weeks=1),\n",
        "        log_max_date - pd.Timedelta(weeks=4)\n",
        "        ]\n",
        "    col_names = [\n",
        "        'has_repeated',\n",
        "        'has_repeated_last_week',\n",
        "        'has_repeated_last_month'\n",
        "    ]\n",
        "    for col_name, date_split in zip(col_names, date_splits):  \n",
        "        new_col = support_log.query('event_date >= @date_split').groupby('contract_id')['event_type'].apply(\n",
        "            lambda x : ((x.shape[0] - x.nunique()) > 0) * 1\n",
        "            ).rename(col_name)\n",
        "        X = pd.merge(X, new_col, on='contract_id', how='left')\n",
        "        X[col_name] = X[col_name].fillna(0).astype('category')\n",
        "\n",
        "    # Наличие обращений по самым популярным и наименее популярным темам\n",
        "    type_counts = support_log.groupby('event_type')['event_type']\\\n",
        "        .count()\\\n",
        "        .sort_values(ascending=False)\n",
        "    top_5_types = set(type_counts.head(3).index.values)\n",
        "    bottom_types = set(type_counts[type_counts <= 5].index.values)\n",
        "    X['has_top_types'] = support_log.groupby('contract_id')['event_type']\\\n",
        "        .apply(lambda x : (set(x) & top_5_types) != set()) * 1\n",
        "    X['has_bottom_types'] = support_log.groupby('contract_id')['event_type']\\\n",
        "        .apply(lambda x : (set(x) & bottom_types) != set()) * 1\n",
        "    X[['has_top_types', 'has_bottom_types']] = X[['has_top_types', 'has_bottom_types']].fillna(0).astype('category')\n",
        "\n",
        "    del counts, days_from_last_request, days_from_first_request, n_types,\\\n",
        "        date_splits, col_names, type_counts,\\\n",
        "        top_5_types, bottom_types\n",
        "    gc.collect()\n",
        "    return X\n",
        "\n",
        "def add_meta_features(events, X, y=None, vectorizer=None, estimator=None, n_folds=2):\n",
        "    index = X.index.values\n",
        "    corpus = events.sort_values(by=['contract_id', 'date'])\\\n",
        "        .groupby('contract_id')['event']\\\n",
        "        .apply(lambda x : '__'.join(x))\\\n",
        "        .reindex(index)\\\n",
        "        .fillna('Nothing')\n",
        "    meta = pd.Series(index=index, dtype='float', name='meta')\n",
        "    if ((not vectorizer) and (not estimator) and (y is not None)):\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            tokenizer = lambda s: s.split('__'),\n",
        "            **META_VEC_PARAMS\n",
        "            )\n",
        "        estimator = SGDClassifier(\n",
        "            loss='log',\n",
        "            random_state=RANDOM_STATE,\n",
        "            class_weight='balanced',\n",
        "            **META_ESTIMATOR_PARAMS\n",
        "        )\n",
        "\n",
        "        events_vec = vectorizer.fit_transform(corpus)        \n",
        "        np.random.seed(RANDOM_STATE)\n",
        "        rand_indices = np.random.permutation(np.arange(X.shape[0]))\n",
        "        s = int(X.shape[0] / n_folds)\n",
        "        for i in range(n_folds):\n",
        "            if i < (n_folds-1):\n",
        "                transform_fold = rand_indices[s * i : s * (i+1)]\n",
        "            else:\n",
        "                transform_fold = rand_indices[s * i :]\n",
        "            fit_fold = list(set(rand_indices) - set(transform_fold))\n",
        "            estimator.fit(events_vec[fit_fold, :], y.iloc[fit_fold])  \n",
        "            meta.iloc[transform_fold] =\\\n",
        "                estimator.predict_proba(events_vec[transform_fold, :])[:, 1].flatten()\n",
        "        estimator.fit(events_vec, y)\n",
        "\n",
        "    elif ((vectorizer) and (estimator) and (y is None)):\n",
        "        events_vec = vectorizer.transform(corpus)\n",
        "        meta.iloc[:] = estimator.predict_proba(events_vec)[:, 1].flatten()\n",
        "\n",
        "    X = pd.merge(X, meta, how='left', left_on='contract_id', right_index=True)\n",
        "    del meta, corpus, events_vec\n",
        "    gc.collect()\n",
        "    return X, vectorizer, estimator\n",
        "\n",
        "def add_pca_features(events, X, y=None, n_components=0.95, vectorizer=None, estimator=None):\n",
        "    index = X.index.values\n",
        "    corpus = events.sort_values(by=['contract_id', 'date'])\\\n",
        "        .groupby('contract_id')['event']\\\n",
        "        .apply(lambda x : '__'.join(x))\\\n",
        "        .reindex(index)\\\n",
        "        .fillna('Nothing')\n",
        "\n",
        "    if ((not vectorizer) and (not estimator)):\n",
        "        vectorizer = TfidfVectorizer(**PCA_VEC_PARAMS)\n",
        "        events_vec = vectorizer.fit_transform(corpus)\n",
        "        estimator = PCA(**PCA_PARAMS, n_components=n_components)\n",
        "        events_pca = estimator.fit_transform(events_vec.toarray())\n",
        "\n",
        "    elif ((vectorizer) and (estimator)):\n",
        "        events_vec = vectorizer.transform(corpus)\n",
        "        events_pca = estimator.transform(events_vec.toarray())\n",
        "\n",
        "    events_pca = pd.DataFrame(\n",
        "        events_pca,\\\n",
        "        index=index,\\\n",
        "        columns=[f'pca_{i+1}' for i in range(estimator.n_components_)]\n",
        "        )\n",
        "    X = pd.merge(X, events_pca, how='left', left_on='contract_id', right_index=True)\n",
        "\n",
        "    del corpus, events_vec, events_pca\n",
        "    gc.collect()\n",
        "    return X, vectorizer, estimator\n",
        "    \n",
        "def scoring(estimator, X, y_true):\n",
        "  preds = estimator.predict(X)\n",
        "  return recall_score(y_true, preds, average='macro')\n",
        "\n",
        "def validate(model, X, y, cv, random_state=RANDOM_STATE):    \n",
        "    cv_results = cross_validate(\n",
        "        model,\n",
        "        X,\n",
        "        y,\n",
        "        cv=cv,\n",
        "        n_jobs=-1,\n",
        "        scoring=scoring,\n",
        "        return_train_score=True\n",
        "        )\n",
        "    print('test: ', cv_results['test_score'].mean(), cv_results['test_score'].std())\n",
        "    print('train: ', cv_results['train_score'].mean(), cv_results['train_score'].std())\n",
        "    print('diff: ',\n",
        "        (cv_results['train_score'] - cv_results['test_score']).mean(),\n",
        "        (cv_results['train_score'] - cv_results['test_score']).std()\n",
        "        )\n",
        "    return cv_results['test_score']"
      ],
      "metadata": {
        "id": "Rqe0-BOqeG6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Валидация"
      ],
      "metadata": {
        "id": "LLP98HQSC-zZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = train_df['blocked']"
      ],
      "metadata": {
        "id": "E9s72Eke5yPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = add_payment_features(train_df.drop('blocked', axis=1))\n",
        "X_train = add_url_features(X_train, competitors)\n",
        "X_train = add_support_features(X_train, log)\n",
        "X_train, vectorizer_meta, estimator_meta = add_meta_features(events, X_train, train_df['blocked'], n_folds=30)\n",
        "X_train, vectorizer_pca, estimator_pca = add_pca_features(events, X_train, train_df['blocked'], 12)"
      ],
      "metadata": {
        "id": "nA0kHdCn2Cjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Параметры по умолчанию\n",
        "model = LGBMClassifier(\n",
        "    objective='binary', class_weight='balanced',\\\n",
        "    random_state=RANDOM_STATE,\\\n",
        "    n_jobs=-1, importance_type='gain'\n",
        ")\n",
        "lof = LocalOutlierFactor(n_jobs=-1)\n",
        "mask = clean(X_train, lof)\n",
        "cv_split = StratifiedKFold(10, shuffle=True, random_state=RANDOM_STATE)\n",
        "validate(model, X_train.loc[mask, :], y_train.loc[mask], cv_split, RANDOM_STATE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CYlV_8Lszcz",
        "outputId": "cf150caa-4017-4b94-9976-f1d669ab488a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.07%, 427 штук выбросов\n",
            "test:  0.6911667434530583 0.03401115590689564\n",
            "train:  0.9087515663419602 0.003728271459332209\n",
            "diff:  0.21758482288890185 0.03600532844189617\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.67971919, 0.6454266 , 0.69057429, 0.70518509, 0.69117647,\n",
              "       0.68196014, 0.67808043, 0.78347307, 0.6816565 , 0.67441565])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Файл с признаками сохраняется для подбора гиперпараметров"
      ],
      "metadata": {
        "id": "hCRW4FlMBkrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pd.concat([X_train, y_train], axis=1).to_csv('X_for_tuning.csv')\n",
        "# !git add 'X_for_tuning.csv'\n",
        "# !git commit -m 'updated features for tuning'\n",
        "# !git push origin "
      ],
      "metadata": {
        "id": "BuK4967_uy-K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "144f7362-944a-4703-e157-c0a33cf5b3da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[master 92c8dfa] updated features for tuning\n",
            " 1 file changed, 5992 insertions(+), 5992 deletions(-)\n",
            "Counting objects: 3, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects: 100% (3/3), done.\n",
            "Writing objects: 100% (3/3), 79.29 KiB | 1.47 MiB/s, done.\n",
            "Total 3 (delta 2), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "remote: \n",
            "remote: Create a pull request for 'master' on GitHub by visiting:\u001b[K\n",
            "remote:      https://github.com/iamzager/Karelia_churn/pull/new/master\u001b[K\n",
            "remote: \n",
            "To https://github.com/iamzager/Karelia_churn.git\n",
            " * [new branch]      master -> master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = LGBMClassifier(\n",
        "    objective='binary', class_weight='balanced',\\\n",
        "    random_state=RANDOM_STATE,\\\n",
        "    n_jobs=-1, importance_type='gain', **MODEL_BEST_PARAMS\n",
        ")"
      ],
      "metadata": {
        "id": "Sdlbxl4qgo6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Параметры после тюнинга\n",
        "cv_split = StratifiedKFold(10, shuffle=True, random_state=RANDOM_STATE)\n",
        "best_lof = LocalOutlierFactor(**LOF_BEST_PARAMS, n_jobs=-1)\n",
        "best_mask = clean(X_train, best_lof)\n",
        "cv_results = validate(best_model, X_train.loc[best_mask, :], y_train.loc[best_mask], cv_split, RANDOM_STATE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlsYXPgwf-kp",
        "outputId": "02bf9be6-d5ef-4047-c935-385a7776b1fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.05%, 278 штук выбросов\n",
            "test:  0.7520706961421089 0.027369368182338046\n",
            "train:  0.7881648271101672 0.0027332833881615823\n",
            "diff:  0.03609413096805832 0.02929964836663476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Прогноз"
      ],
      "metadata": {
        "id": "z9Q9mEWputms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = add_payment_features(sample.set_index('contract_id').drop('blocked', axis=1))\n",
        "X_test = add_url_features(X_test, competitors)\n",
        "X_test = add_support_features(X_test, log)\n",
        "X_test, _, _ = add_meta_features(events, X_test, None, vectorizer_meta, estimator_meta)\n",
        "X_test, _, _ = add_pca_features(events, X_test, None, None, vectorizer_pca, estimator_pca)"
      ],
      "metadata": {
        "id": "ylu0-u_M6eW9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dd5cb21-ece6-445d-d813-c5d649ef68e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/arraylike.py:364: RuntimeWarning: divide by zero encountered in log1p\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.fit(X_train.loc[best_mask, :], y_train.loc[best_mask])\n",
        "preds = pd.Series(best_model.predict(X_test), index=X_test.index)\n",
        "preds = pd.merge(sample.drop('blocked', axis=1), preds.rename('blocked'), on='contract_id')\n",
        "preds.to_csv(PREDS_FILE_NAME, index=False)"
      ],
      "metadata": {
        "id": "wRIh_4k_2ChS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git add $PREDS_FILE_NAME\n",
        "# !git commit -m 'updated predictions'\n",
        "# !git push origin"
      ],
      "metadata": {
        "id": "bkqaCDC4wgwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L4AJPVCS6_UJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
